{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6323141c-4c2d-46e0-8959-62e6db8a5b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Conv2D, Conv2DTranspose, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 2e-4\n",
    "LAMBDA = 100  # Regularization parameter for cycle-consistency loss\n",
    "\n",
    "# Directories\n",
    "TRAIN_DIR = '../data/processed/satellite_to_map/train/'\n",
    "TEST_DIR = '../data/processed/satellite_to_map/test/'\n",
    "CHECKPOINT_DIR = './checkpoints/'\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_image_pair(satellite_path, map_path):\n",
    "    satellite = tf.io.read_file(satellite_path)\n",
    "    satellite = tf.image.decode_jpeg(satellite, channels=3)\n",
    "    satellite = tf.image.resize(satellite, (256, 256))\n",
    "    satellite = tf.cast(satellite, tf.float32) / 127.5 - 1.0\n",
    "    \n",
    "    map_image = tf.io.read_file(map_path)\n",
    "    map_image = tf.image.decode_jpeg(map_image, channels=3)\n",
    "    map_image = tf.image.resize(map_image, (256, 256))\n",
    "    map_image = tf.cast(map_image, tf.float32) / 127.5 - 1.0\n",
    "    \n",
    "    return satellite, map_image\n",
    "\n",
    "# Function to build the generator model\n",
    "def build_generator(input_shape=(256, 256, 3), output_channels=3):\n",
    "    inputs = Input(shape=input_shape, name='input_image')\n",
    "    \n",
    "    # Encoder\n",
    "    down1 = Conv2D(64, (4, 4), strides=(2, 2), padding='same', activation='relu')(inputs)\n",
    "    down2 = Conv2D(128, (4, 4), strides=(2, 2), padding='same', activation='relu')(down1)\n",
    "    down3 = Conv2D(256, (4, 4), strides=(2, 2), padding='same', activation='relu')(down2)\n",
    "    down4 = Conv2D(512, (4, 4), strides=(2, 2), padding='same', activation='relu')(down3)\n",
    "    \n",
    "    # Bottleneck\n",
    "    bottleneck = Conv2D(512, (4, 4), strides=(2, 2), padding='same', activation='relu')(down4)\n",
    "    \n",
    "    # Decoder\n",
    "    up1 = Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same', activation='relu')(bottleneck)\n",
    "    merge1 = Concatenate()([up1, down3])\n",
    "    \n",
    "    up2 = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', activation='relu')(merge1)\n",
    "    merge2 = Concatenate()([up2, down2])\n",
    "    \n",
    "    up3 = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', activation='relu')(merge2)\n",
    "    merge3 = Concatenate()([up3, down1])\n",
    "    \n",
    "    outputs = Conv2DTranspose(output_channels, (4, 4), strides=(2, 2), padding='same', activation='tanh')(merge3)\n",
    "    \n",
    "    generator = Model(inputs, outputs, name='generator')\n",
    "    return generator\n",
    "\n",
    "# Function to build the discriminator model\n",
    "def build_discriminator(input_shape=(256, 256, 3)):\n",
    "    input_image = Input(shape=input_shape, name='input_image')\n",
    "    target_image = Input(shape=input_shape, name='target_image')\n",
    "    \n",
    "    combined_input = Concatenate()([input_image, target_image])\n",
    "    \n",
    "    conv1 = Conv2D(64, (4, 4), strides=(2, 2), padding='same')(combined_input)\n",
    "    conv1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, (4, 4), strides=(2, 2), padding='same')(conv1)\n",
    "    conv2 = LeakyReLU(alpha=0.2)(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(256, (4, 4), strides=(2, 2), padding='same')(conv2)\n",
    "    conv3 = LeakyReLU(alpha=0.2)(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(512, (4, 4), padding='same')(conv3)\n",
    "    conv4 = LeakyReLU(alpha=0.2)(conv4)\n",
    "    \n",
    "    output = Conv2D(1, (4, 4), padding='same')(conv4)\n",
    "    \n",
    "    discriminator = Model(inputs=[input_image, target_image], outputs=output, name='discriminator')\n",
    "    return discriminator\n",
    "\n",
    "# Function to calculate generator loss\n",
    "def generator_loss(discriminator_real_outputs, generated_outputs, target_images):\n",
    "    adversarial_loss = BinaryCrossentropy(from_logits=True)(tf.ones_like(discriminator_real_outputs), generated_outputs)\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target_images - generated_outputs))\n",
    "    total_gen_loss = adversarial_loss + (LAMBDA * l1_loss)\n",
    "    return total_gen_loss\n",
    "\n",
    "# Function to calculate discriminator loss\n",
    "def discriminator_loss(discriminator_real_outputs, discriminator_generated_outputs):\n",
    "    real_loss = BinaryCrossentropy(from_logits=True)(tf.ones_like(discriminator_real_outputs), discriminator_real_outputs)\n",
    "    generated_loss = BinaryCrossentropy(from_logits=True)(tf.zeros_like(discriminator_generated_outputs), discriminator_generated_outputs)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss\n",
    "\n",
    "# Function for training step\n",
    "def train_step(generator, discriminator, input_images, target_images, generator_optimizer, discriminator_optimizer, training=True):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(input_images, training=training)\n",
    "        \n",
    "        discriminator_real_outputs = discriminator([input_images, target_images], training=training)\n",
    "        discriminator_generated_outputs = discriminator([input_images, generated_images], training=training)\n",
    "        \n",
    "        gen_loss = generator_loss(discriminator_real_outputs, discriminator_generated_outputs, target_images)\n",
    "        disc_loss = discriminator_loss(discriminator_real_outputs, discriminator_generated_outputs)\n",
    "    \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# Function to load and prepare dataset\n",
    "def load_data(train_dir, test_dir):\n",
    "    train_satellite_paths = [os.path.join(train_dir, 'satellite_images', filename) for filename in os.listdir(os.path.join(train_dir, 'satellite_images'))]\n",
    "    train_map_paths = [os.path.join(train_dir, 'map_images', filename) for filename in os.listdir(os.path.join(train_dir, 'map_images'))]\n",
    "    test_satellite_paths = [os.path.join(test_dir, 'satellite_images', filename) for filename in os.listdir(os.path.join(test_dir, 'satellite_images'))]\n",
    "    test_map_paths = [os.path.join(test_dir, 'map_images', filename) for filename in os.listdir(os.path.join(test_dir, 'map_images'))]\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_satellite_paths, train_map_paths))\n",
    "    train_dataset = train_dataset.shuffle(len(train_satellite_paths)).map(load_image_pair, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
    "    \n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_satellite_paths, test_map_paths))\n",
    "    test_dataset = test_dataset.map(load_image_pair, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# Main training function\n",
    "def train():\n",
    "    # Build generator and discriminator\n",
    "    generator = build_generator()\n",
    "    discriminator = build_discriminator()\n",
    "    \n",
    "    # Optimizers\n",
    "    generator_optimizer = Adam(learning_rate=LEARNING_RATE, beta_1=0.5)\n",
    "    discriminator_optimizer = Adam(learning_rate=LEARNING_RATE, beta_1=0.5)\n",
    "    \n",
    "    # Load data\n",
    "    train_dataset, test_dataset = load_data(TRAIN_DIR, TEST_DIR)\n",
    "    \n",
    "    # Checkpoint\n",
    "    checkpoint_prefix = os.path.join(CHECKPOINT_DIR, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                     discriminator_optimizer=discriminator_optimizer,\n",
    "                                     generator=generator,\n",
    "                                     discriminator=discriminator)\n",
    "    \n",
    "    # Metrics\n",
    "    gen_loss_metric = Mean(name='gen_loss')\n",
    "    disc_loss_metric = Mean(name='disc_loss')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        # Reset metrics\n",
    "        gen_loss_metric.reset_states()\n",
    "        disc_loss_metric.reset_states()\n",
    "        \n",
    "        # Training\n",
    "        for batch_num, (input_images, target_images) in enumerate(train_dataset):\n",
    "            gen_loss, disc_loss\n",
    "            # Perform one training step\n",
    "            gen_loss, disc_loss = train_step(generator, discriminator, input_images, target_images,\n",
    "                                             generator_optimizer, discriminator_optimizer)\n",
    "            \n",
    "            # Update metrics\n",
    "            gen_loss_metric.update_state(gen_loss)\n",
    "            disc_loss_metric.update_state(disc_loss)\n",
    "            \n",
    "            # Print training metrics\n",
    "            if (batch_num + 1) % 100 == 0:\n",
    "                print(f\"Batch {batch_num + 1}, Generator Loss: {gen_loss_metric.result()}, Discriminator Loss: {disc_loss_metric.result()}\")\n",
    "        \n",
    "        # Save checkpoint (every epoch)\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "        \n",
    "        # Validation\n",
    "        for batch_num, (input_images, target_images) in enumerate(test_dataset):\n",
    "            val_gen_loss, val_disc_loss = train_step(generator, discriminator, input_images, target_images,\n",
    "                                                    generator_optimizer, discriminator_optimizer, training=False)\n",
    "        \n",
    "        # Print validation metrics\n",
    "        print(f\"Validation - Generator Loss: {gen_loss_metric.result()}, Discriminator Loss: {disc_loss_metric.result()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
